---
layout: post
category: project
mytype: course
title: Identify Proteins Critical to Learning in a Mouse Model of Down Syndrome
title2: A Course-Based Project
mydate: Dec. 2018
prointro: A course-based project. Reproduced experiments in the <a href="https://www.ncbi.nlm.nih.gov/pubmed/26111164">original paper</a> and tried other statistical techniques to cluster two kinds of mice as well as find out important proteins. Theproject is finished with Jiliang Ma and Qinyuan Wei, and is open-source on <a href="https://github.com/brycenycuu/Identify-Proteins-Critical-to-Learning">GitHub</a>.
---

### What is the problem?

In the original paper *<a href="https://www.ncbi.nlm.nih.gov/pubmed/26111164">Self-Organizing Feature Maps Identify Proteins Critical to Learning in a Mouse Model of Down Syndrome</a>*, authors use <a href="https://en.wikipedia.org/wiki/Self-organizing_map">Self-Organizing Maps</a> (SOM) and <a href="https://en.wikipedia.org/wiki/Mannâ€“Whitney_U_test">Wilcoxon rank-sum test</a> to do a large number experiments and validations, which can derive the biological conclutions below.

> Results suggest that the application of SOM to new experimental data sets of complex protein profiles can be used to identify common critical protein responses, which in turn may aid in identifying potentially more effective drug targets.


### What did we do?

#### Reproduction

We reproduced part of the experiments in the article, and the results match the corresponding results in the paper.

<div class="protem">
	<div align="middle">
		<table>
		<tr><td><img src="/img/mouse/paper77.png" width="320"></td>
		<td><img src="/img/mouse/our77.png" width="320"></td></tr>
		</table>
	</div>
	<div class="notes">
		Result of SOM under all 77 proteins in the original paper (left) and our experiment (right).
	</div>
</div>

Under all 77 proteins, the learning mice (in yellow and green) can be seperated from the non-learning mice (in orange and brown). Then we use <a href="https://en.wikipedia.org/wiki/Depth-first_search">depth-first search</a> (DFS) to distinguish the largest cluster of each kind of mice.

<div class="protem">
	<div align="middle">
		<img src="/img/mouse/our-largecluster.png" width="320">
	</div>
	<div class="notes">
		The largest cluster of each kind of mice.
	</div> 
</div>

Then Wilcoxon rank-sum test is adopted to choosing the significantly-different proteins between several pairs of kinds of mice. The proteins exist in all the subsets are considered as important proteins. Since the platforms are different and the results are quite random, we then use the 11 important proteins derived by the paper for the following experiments.

<div class="protem">
	<div align="middle">
		<img src="/img/mouse/paper11.png" width="310"><img src="/img/mouse/our11.png" width="310">
	</div>
	<div align="middle">
		<img src="/img/mouse/paper66.png" width="310"><img src="/img/mouse/our66.png" width="310">
	</div>
	<div class="notes">
		Result of SOM under the selected 11 important proteins (row 1) and other 66 proteins (row 2) in the original paper (column 1) and our experiment (column 2).
	</div>
</div>

It can be found that the 11 important proteins can also distinguish the learning and non-learning mice, but the protein subset of the other 66 proteins cannot. Thus the 11 selected proteins are really important to seperate these two kinds of mice.

#### Our methods

We first try <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">principal component analysis</a> (PCA) and find that the first two dimentions only reach 40% explained variance, which indicates PCA is not a good idea to find which protein is the important indicator of a character in this biological topic.

<div class="protem">
	<div align="middle">
		<img src="/img/mouse/pca.png" width="500">
	</div>
	<div class="notes">
		Clustering result under the first two dimensions adopting PCA.
	</div> 
</div>

We then try another two clustering methods, <a href="https://en.wikipedia.org/wiki/K-means_clustering">k-means clustering</a> and <a href="https://en.wikipedia.org/wiki/Hierarchical_clustering">hierarchical clustering</a>, to validate the result under 77/11/66 proteins (11/66 are the important and non-important proteins derived by the paper), as well as to find important proteins using forward stepwise selection and backward stepwise selection. Since it is a bi-clustering problem, it is easy to calculate the error rate as the benchmark.

Here we only show the result derived by k-means clustering method. The error rate under all 77 proteins is higher than that under 11 important proteins, but lower that that under the other 66 proteins. And both forward and backward stepwise selections get good results, as is shown in the graph below. The important proteins found by K-means matches the results in the paper.

<div class="protem">
	<div align="middle">
		<img src="/img/mouse/kmeans-forward.png" width="480">
		<img src="/img/mouse/kmeans-backward.png" width="480">
	</div>
	<div class="notes">
		Forward stepwise selection (above) and backward stepwise selection (below) by K-means clustering method.
	</div> 
</div>

However, the hierarchical clustering does not get such good results. The error rate under three conditions are all really high, and both of the lowest error rates in forward and backward stepwise selection do not reach 0. We can tell the hierarchical method performs not well.

Our future work includes applying SOM to other clustering problems, and finding some biomedical revelation based on our K-means clustering method.

&nbsp; 

***Resources:***   
*<a href="https://www.ncbi.nlm.nih.gov/pubmed/26111164">Original paper</a>*
